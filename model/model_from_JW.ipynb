{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "param_distribs={'C': randint(low=0.001, high=100)}\n",
    "model = RandomizedSearchCV(LogisticRegression(max_iter=5000), param_distributions=param_distribs, n_iter=n_iter, cv=cv, random_state=random_state, scoring = scoring, n_jobs = -1)\n",
    "model.fit(X_train, y_train)\n",
    "print(model.best_params_, model.best_score_)\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "param_distribs = {'n_neighbors': randint(low=1, high=80)}\n",
    "model = RandomizedSearchCV(KNeighborsClassifier(), param_distributions=param_distribs, n_iter=n_iter, cv=cv, random_state=random_state, scoring = scoring, n_jobs = -1)\n",
    "model.fit(X_train, y_train)\n",
    "print(model.best_params_, model.best_score_)\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "param_distribs = {'var_smoothing': randint(low=0, high=110)}\n",
    "model = RandomizedSearchCV(GaussianNB(), param_distributions=param_distribs, n_iter=n_iter, cv=cv, random_state=random_state, scoring = scoring, n_jobs = -1)\n",
    "model.fit(X_train, y_train)\n",
    "print(model.best_params_, model.best_score_)   \n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "param_distribs={'hidden_layer_sizes': randint(low=10, high=110), 'solver': ['sgd', 'adam'], 'activation': ['tanh', 'relu']}\n",
    "model = RandomizedSearchCV(MLPClassifier(), param_distributions=param_distribs, n_iter=n_iter, cv=cv, random_state=random_state, scoring = scoring, n_jobs = -1)\n",
    "model.fit(X_train, y_train)\n",
    "print(model.best_params_, model.best_score_)   \n",
    "\n",
    "from sklearn.svm import SVC\n",
    "param_distribs = {'kernel' : ['rbf', 'poly'], 'C' : randint(low = 0.001,  high = 100), 'gamma' : randint(low = 0.001, high = 100)}  \n",
    "model = RandomizedSearchCV(SVC(), param_distributions=param_distribs, n_iter=n_iter, cv=cv, random_state=random_state, scoring = scoring, n_jobs = -1)\n",
    "model.fit(X_train, y_train)\n",
    "print(model.best_params_, model.best_score_)   \n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "param_distribs = {'max_depth': randint(low=1, high=20), 'min_samples_leaf': randint(low=1, high=50)}\n",
    "model = RandomizedSearchCV(DecisionTreeClassifier(), param_distributions=param_distribs, n_iter=n_iter, cv=cv, random_state=random_state, scoring = scoring, n_jobs = -1)\n",
    "model.fit(X_train, y_train)\n",
    "print(model.best_params_, model.best_score_)   \n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "param_distribs = {'n_estimators': randint(low=100, high=1000), 'max_features': ['auto', 'sqrt', 'log2']}\n",
    "model = RandomizedSearchCV(RandomForestClassifier(), param_distributions=param_distribs, n_iter=n_iter, cv=cv, random_state=random_state, scoring = scoring, n_jobs = -1)\n",
    "model.fit(X_train, y_train)\n",
    "print(model.best_params_, model.best_score_)   \n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "param_distribs = {'n_estimators': randint(low=100, high=1000), 'max_depth': randint(low=1, high=110)}\n",
    "model = RandomizedSearchCV(XGBClassifier(), param_distributions=param_distribs, n_iter=n_iter, cv=cv, random_state=random_state, scoring = scoring, n_jobs = -1)\n",
    "model.fit(X_train, y_train)\n",
    "print(model.best_params_, model.best_score_)   \n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "param_distribs = {'n_estimators': randint(low=100, high=1000), 'max_depth': randint(low=1, high=20)}\n",
    "model = RandomizedSearchCV(LGBMClassifier(), param_distributions=param_distribs, n_iter=n_iter, cv=cv, random_state=random_state, scoring = scoring, n_jobs = -1)\n",
    "model.fit(X_train, y_train)\n",
    "print(model.best_params_, model.best_score_)   \n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "param_distribs = {'n_estimators': randint(low=100, high=1000)}\n",
    "model = RandomizedSearchCV(AdaBoostClassifier(), param_distributions=param_distribs, n_iter=n_iter, cv=cv, random_state=random_state, scoring = scoring, n_jobs = -1)\n",
    "model.fit(X_train, y_train)\n",
    "print(model.best_params_, model.best_score_)   \n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "param_distribs = {'iterations': randint(low=2, high=1000)}\n",
    "model = RandomizedSearchCV(CatBoostClassifier(), param_distributions=param_distribs, n_iter=n_iter, cv=cv, random_state=random_state, scoring = scoring, n_jobs = -1)\n",
    "model.fit(X_train, y_train)\n",
    "print(model.best_params_, model.best_score_)   \n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "param_distribs = {'n_estimators': randint(low=100, high=1000), 'max_depth': randint(low=1, high=20)}\n",
    "model = RandomizedSearchCV(GradientBoostingClassifier(), param_distributions=param_distribs, n_iter=n_iter, cv=cv, random_state=random_state, scoring = scoring, n_jobs = -1)\n",
    "model.fit(X_train, y_train)\n",
    "print(model.best_params_, model.best_score_)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "def object(trial):\n",
    "    params= {\n",
    "        'n_neighbors':trial.suggest_int(\"n_neighbors\", 3, 61, step=2),\n",
    "        'weights': trial.suggest_categorical('weights', [\"uniform\", \"distance\"]),\n",
    "        'algorithm': trial.suggest_categorical('algorithm', ['auto', 'ball_tree', 'kd_tree', 'brute']),\n",
    "        'leaf_size':trial.suggest_int(\"leaf_size\", 1, 60),\n",
    "        'p': trial.suggest_categorical('p', [1, 2]),\n",
    "        'metric': trial.suggest_categorical('metric', ['euclidean', 'manhattan', 'minkowski']),\n",
    "        }\n",
    "    model = KNeighborsClassifier(n_jobs=-1, **params)\n",
    "    score = cross_val_score(model, X_train, y_train, cv=cv, scoring = scoring, n_jobs = -1)\n",
    "    accuracy = score.mean()\n",
    "    return accuracy\n",
    "study = optuna.create_study(direction = direction)\n",
    "study.optimize(object, n_trials=n_trials)\n",
    "print(study.best_trial.params)\n",
    "print(study.best_trial.values)\n",
    "# optimization\n",
    "opt = KNeighborsClassifier(n_jobs = -1, **study.best_trial.params)\n",
    "opt.fit(X_train, y_train)\n",
    "print(accuracy_score(y_test, opt.predict(X_test)))\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "def object(trial):\n",
    "    params = {\n",
    "        \"n_estimators\" : trial.suggest_int(\"n_estimators\", 50,500),\n",
    "        # \"criterion\" : trial.suggest_categorical(\"criterion\", [\"gini\",\"entropym\"]),\n",
    "        \"max_depth\" : trial.suggest_int(\"max_depth\", 1, 110),\n",
    "        'min_samples_split' : trial.suggest_int ('min_samples_split', 1, 100),\n",
    "        \"min_samples_leaf\":trial.suggest_int(\"min_samples_leaf\", 1, 60), \n",
    "        \"max_features\" : trial.suggest_int(\"max_features\", 1, X_train.shape[1]),\n",
    "        \"max_leaf_nodes\":trial.suggest_int(\"max_leaf_nodes\", 2, 500)            \n",
    "    }\n",
    "    model = RandomForestClassifier(n_jobs = -1, random_state = random_state, **params)\n",
    "    score = cross_val_score(model, X_train, y_train, cv=cv, scoring = scoring, n_jobs = -1)\n",
    "    accuracy = score.mean()\n",
    "    return accuracy\n",
    "study = optuna.create_study(direction = direction)\n",
    "study.optimize(object, n_trials=n_trials)\n",
    "print(study.best_trial.params)\n",
    "print(study.best_trial.values)\n",
    "# optimization\n",
    "opt = RandomForestClassifier(n_jobs = -1, random_state = random_state, **study.best_trial.params)\n",
    "opt.fit(X_train, y_train)\n",
    "print(accuracy_score(y_test, opt.predict(X_test)))\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "def object(trial):\n",
    "    params = {\n",
    "        'kernel': trial.suggest_categorical('kernel', ['linear','rbf','sigmoid']),\n",
    "        'C': trial.suggest_float('C', 1e+0, 1e+2/2, log=True),\n",
    "        'degree' :trial.suggest_int(\"degree\", 0,6),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 0.001, 3.0, log=True)\n",
    "    }\n",
    "    model = SVC(random_state = random_state, **params)\n",
    "    score = cross_val_score(model, X_train, y_train, cv=cv, scoring = scoring, n_jobs = -1)\n",
    "    accuracy = score.mean()\n",
    "    return accuracy\n",
    "study = optuna.create_study(direction = direction)\n",
    "study.optimize(object, n_trials=n_trials)\n",
    "print(study.best_trial.params)\n",
    "print(study.best_trial.values)\n",
    "# optimization\n",
    "opt = SVC(random_state = random_state, **study.best_trial.params)\n",
    "opt.fit(X_train, y_train)\n",
    "print(accuracy_score(y_test, opt.predict(X_test)))\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "def object(trial):\n",
    "    params = {\n",
    "        \"n_estimators\" : trial.suggest_int(\"n_estimators\", 50,500),\n",
    "        \"learning_rate\" : trial.suggest_float(\"learning_rate\", 0.0001, 0.3, log=True)          \n",
    "    }\n",
    "    model = AdaBoostClassifier(random_state = random_state, **params)\n",
    "    score = cross_val_score(model, X_train, y_train, cv=cv, scoring = scoring, n_jobs = -1)\n",
    "    accuracy = score.mean()\n",
    "    return accuracy\n",
    "study = optuna.create_study(direction = direction)\n",
    "study.optimize(object, n_trials=n_trials)\n",
    "print(study.best_trial.params)\n",
    "print(study.best_trial.values)\n",
    "# optimization\n",
    "opt = AdaBoostClassifier(random_state = random_state, **study.best_trial.params)\n",
    "opt.fit(X_train, y_train)\n",
    "print(accuracy_score(y_test, opt.predict(X_test)))\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "def object(trial):\n",
    "    params = {\n",
    "        \"booster\" : trial.suggest_categorical(\"booster\", [\"gbtree\",\"gblinear\",\"dart\"]),\n",
    "        \"max_depth\" : trial.suggest_int(\"max_depth\", 1, 100),\n",
    "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\",2,15),\n",
    "        \"subsample\" : trial.suggest_float(\"subsample\", 0.6, 1, step =0.05),\n",
    "        \"colsample_bytree\":trial.suggest_float(\"colsample_bytree\", 0.1,1, step =0.05),\n",
    "        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.5, 1.0),\n",
    "        \"colsample_bynode\": trial.suggest_float(\"colsample_bynode\", 0.5, 1.0),        \n",
    "        \"reg_lambda\" : trial.suggest_float(\"reg_alpha\", 0.01, 1, log=True),\n",
    "        \"reg_alpha\" : trial.suggest_float(\"reg_alpha\", 0.01, 1, log=True),\n",
    "        \"scale_pos_weight\": trial.suggest_int('scale_pos_weight', 1, 30),\n",
    "        \"n_estimators\" : trial.suggest_int(\"n_estimators\", 50,500),\n",
    "        \"learning_rate\" : trial.suggest_float(\"learning_rate\", 0.0001, 0.003, log=True),\n",
    "        \"colsample_bynode\": trial.suggest_float(\"colsample_bynode\", 0.5, 1.0),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 0.1, 1.0, log=True) #\"eta\": trial.suggest_loguniform(\"eta\",1e-2,0.1)\n",
    "        #'num_parallel_tree': trial.suggest_int(\"num_parallel_tree\", 1, 500) if append, slowly   \n",
    "    }\n",
    "    model = XGBClassifier(n_jobs = -1, random_state = random_state, **params, objective = objective, eval_metric  =\"error\", tree_method='exact') #binary:logistic, multi:softmax, multi:softprob, #gpu\n",
    "    score = cross_val_score(model, X_train, y_train, cv=cv, scoring = scoring, n_jobs = -1)\n",
    "    accuracy = score.mean()\n",
    "    return accuracy\n",
    "study = optuna.create_study(direction = direction)\n",
    "study.optimize(object, n_trials=n_trials)\n",
    "print(study.best_trial.params)\n",
    "print(study.best_trial.values)\n",
    "# optimization\n",
    "opt = XGBClassifier(n_jobs = -1, random_state = random_state, **study.best_trial.params, objective = objective, eval_metric  =\"error\", tree_method='exact')\n",
    "opt.fit(X_train, y_train)\n",
    "print(accuracy_score(y_test, opt.predict(X_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
